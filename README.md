# ðŸ“„ RAG Chatbot for Internal Document Question Answering

This project is a **Retrieval-Augmented Generation (RAG)** chatbot designed to answer natural language queries based on internal PDF documents. It uses SentenceTransformers for embeddings, FAISS for vector storage, and the Groq API for fast and efficient LLM-powered answer generation. The UI is built with **Streamlit** for ease of use.

---

## ðŸš€ Demo

ðŸ”— **Live App:** [RAG Chatbot Streamlit App](https://ragchatbotprojectgit-xx4jrh9dcpqunyrrprrkx8.streamlit.app/)


---

## ðŸ§  Features

- Upload internal documents (PDFs)
- Automatic semantic chunking using Sentence Transformers
- Embedding with `BAAI/bge-base-en-v1.5`
- Vector storage with **FAISS**
- Fast responses via **Groq-hosted LLaMA 3** model
- User-friendly interface with **Streamlit**
- Retrieved chunk preview for transparency

---

## ðŸ“‚ Project Structure

```plaintext
rag_chatbot_project/
â”‚
â”œâ”€â”€ app.py                  # Main Streamlit UI and app logic
â”œâ”€â”€ ingest.py               # PDF loading and semantic chunking
â”œâ”€â”€ embed.py                # Embedding and vector store creation
â”œâ”€â”€ query.py                # Querying Groq LLM with context
â”œâ”€â”€ .env                    # Contains your GROQ_API_KEY (not pushed)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ docs/
    â””â”€â”€ text.pdf            # Your input document(s)
